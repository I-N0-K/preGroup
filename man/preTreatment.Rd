% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/preGroup.r, R/preTreatment.r
\name{preTreatment}
\alias{preTreatment}
\title{PreTreatment Function for Building Ensemble Models}
\usage{
preTreatment(
  formula,
  data,
  treatment_indicator,
  alpha.mvs = c(0, 1),
  family = "gaussian",
  ad.alpha = NA,
  ad.penalty = "lambda.min",
  use.grad = TRUE,
  weights,
  type = "both",
  sampfrac = 0.5,
  maxdepth = 3L,
  learnrate = 0.01,
  mtry = Inf,
  ntrees = 500,
  confirmatory = NULL,
  singleconditions = FALSE,
  winsfrac = 0.025,
  normalize = TRUE,
  standardize = FALSE,
  ordinal = TRUE,
  nfolds = 10L,
  tree.control,
  tree.unbiased = TRUE,
  removecomplements = TRUE,
  removeduplicates = TRUE,
  verbose = FALSE,
  par.init = FALSE,
  par.final = FALSE,
  sparse = FALSE,
  ...
)

preTreatment(
  formula,
  data,
  treatment_indicator,
  alpha.mvs = c(0, 1),
  family = "gaussian",
  ad.alpha = NA,
  ad.penalty = "lambda.min",
  use.grad = TRUE,
  weights,
  type = "both",
  sampfrac = 0.5,
  maxdepth = 3L,
  learnrate = 0.01,
  mtry = Inf,
  ntrees = 500,
  confirmatory = NULL,
  singleconditions = FALSE,
  winsfrac = 0.025,
  normalize = TRUE,
  standardize = FALSE,
  ordinal = TRUE,
  nfolds = 10L,
  tree.control,
  tree.unbiased = TRUE,
  removecomplements = TRUE,
  removeduplicates = TRUE,
  verbose = FALSE,
  par.init = FALSE,
  par.final = FALSE,
  sparse = FALSE,
  ...
)
}
\arguments{
\item{formula}{an object of class 'formula' describing the model to be fitted.}

\item{data}{a data frame containing the variables specified in the formula.}

\item{treatment_indicator}{a character string specifying the column used for treatment division.}

\item{alpha.mvs}{a dummay vector of length two, indicating the alpha values used
within  and between group penalization. A value of 1 yields lasso, a value of 0 ridge, a value
between 0 and 1 elastic net. The default uses ridge for feature selection and lasso
for group selection.}

\item{family}{a description of the error distribution to be used in the model. It can be one
of 'gaussian', 'binomial', etc.}

\item{ad.alpha}{optional alpha parameter for adjustment.}

\item{ad.penalty}{the penalty type to be used in adjustment, default is 'lambda.min'.}

\item{use.grad}{logical, indicating whether to use gradients.}

\item{weights}{an optional vector of 'weights' to be used in the fitting process.}

\item{type}{the type of model components to include: 'rules', 'linear', or 'both'.}

\item{sampfrac}{the fraction of the data to be used in each bootstrap sample.}

\item{maxdepth}{maximum depth of the trees in the model.}

\item{learnrate}{learning rate for boosting.}

\item{mtry}{number of variables randomly sampled as candidates at each split.}

\item{ntrees}{number of trees to grow.}

\item{confirmatory}{optional vector of confirmatory conditions.}

\item{singleconditions}{logical, to include only single conditions in rules.}

\item{winsfrac}{the fraction of extreme values to winsorize.}

\item{normalize}{logical, whether to normalize predictor variables.}

\item{standardize}{logical, whether to standardize predictor variables.}

\item{ordinal}{logical, to treat ordered factors as numeric.}

\item{nfolds}{number of folds for cross-validation.}

\item{tree.control}{list of control parameters for tree construction.}

\item{tree.unbiased}{logical, indicating whether to use unbiased trees (conditional inference trees; ctrees) or biased trees (recursive partitionaing; rpart).}

\item{removecomplements}{logical, whether to remove complement rules.}

\item{removeduplicates}{logical, whether to remove duplicate rules.}

\item{verbose}{logical, indicating whether to print detailed output during fitting.}

\item{par.init}{logical, indicating whether to parallelize the initial fit.}

\item{par.final}{logical, indicating whether to parallelize the final model fitting.}

\item{sparse}{logical, to use sparse model matrices.}

\item{...}{additional arguments affecting the model fitting.}
}
\value{
A list containing the fitted model object, the original call, and a classification
of the model rules into types such as 'linear', 'prognostic', and 'prescriptive'.

A list containing the fitted model object, the original call, and a classification
of the model rules into types such as 'linear', 'prognostic', and 'prescriptive'.
}
\description{
This function facilitates the creation of ensemble models by grouping data based on
treatment indicators and other factors. It allows for various configurations and customizations
to tailor the model fitting process to specific needs, including handling of different
statistical family distributions.

This function facilitates the creation of ensemble models by grouping data based on
treatment indicators and other factors. It allows for various configurations and customizations
to tailor the model fitting process to specific needs, including handling of different
statistical family distributions.
}
\examples{
set.seed(123)  # For reproducibility
# Number of rows
n <- 200

# Generate 5 binary variables
binary_vars <- as.data.frame(replicate(5, sample(0:1, n, replace = TRUE)))
names(binary_vars) <- paste0("X", 1:5)

# Generate 10 categorical variables with 3 levels each
categorical_vars <- as.data.frame(replicate(10, sample(letters[1:3], n, replace = TRUE)))
names(categorical_vars) <- paste0("X", 6:15)

# Generate 10 continuous variables
continuous_vars <- as.data.frame(replicate(10, rnorm(n)))
names(continuous_vars) <- paste0("X", 16:25)
# Combine all predictor variables
predictors <- cbind(binary_vars, categorical_vars, continuous_vars)
y <- with(predictors, {
  2 * sin(X16) + 3 * cos(X17) + X1 * X18^2 - X2 * X19 + X3 * X20^3 +
    as.numeric(X1 == 1 & X9 == "b") + as.numeric(X1 == 1 & X16 < 0) +
    as.numeric(X6 == "a") * X21 + as.numeric(X7 == "b") * X22 +
    as.numeric(X8 == "c") * X23 +
    25 * as.numeric(X8 == "c") + 2 * as.numeric(X9 == "b") + X16 + X18 + X19 + rnorm(n)  # Adding some noise
})
#Combine predictors and response into a single data frame
simdata1 <- data.frame(y = y, predictors)
result <- preTreatment(y ~ ., treatment_indicator = "X1", data = simdata1, alpha.mvs = c(0,1))
result

set.seed(123)  # For reproducibility
# Number of rows
n <- 200

# Generate 5 binary variables
binary_vars <- as.data.frame(replicate(5, sample(0:1, n, replace = TRUE)))
names(binary_vars) <- paste0("X", 1:5)

# Generate 10 categorical variables with 3 levels each
categorical_vars <- as.data.frame(replicate(10, sample(letters[1:3], n, replace = TRUE)))
names(categorical_vars) <- paste0("X", 6:15)

# Generate 10 continuous variables
continuous_vars <- as.data.frame(replicate(10, rnorm(n)))
names(continuous_vars) <- paste0("X", 16:25)
# Combine all predictor variables
predictors <- cbind(binary_vars, categorical_vars, continuous_vars)
y <- with(predictors, {
  2 * sin(X16) + 3 * cos(X17) + X1 * X18^2 - X2 * X19 + X3 * X20^3 +
    as.numeric(X1 == 1 & X9 == "b") + as.numeric(X1 == 1 & X16 < 0) +
    as.numeric(X6 == "a") * X21 + as.numeric(X7 == "b") * X22 +
    as.numeric(X8 == "c") * X23 +
    25 * as.numeric(X8 == "c") + 2 * as.numeric(X9 == "b") + X16 + X18 + X19 + rnorm(n)  # Adding some noise
})
#Combine predictors and response into a single data frame
simdata1 <- data.frame(y = y, predictors)
result <- preTreatment(y ~ ., treatment_indicator = "X1", data = simdata1, alpha.mvs = c(0,1))
result

}
